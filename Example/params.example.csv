## Parameters file for the DADA2 pipeline. Currently this is only used to
## forward a few parameters to DADA2's filterAndTrim() (called by
## dada2_nifH_amplicons.R). You can adjust the parameters to try to balance:
##   1. % reads retained during quality filtering. You want a high % so
##      that relative abundnaces can be analyzed.
##   2. Erroneous bases in the ASVs. Your tolerance for errors will depend
##      on how you use the data.  E.g. coarse taxonomic assignments vs.
##      precise sequences for rare ASVs.
##
## The parameters in this example are expected NOT to cause aggressive
## truncation of reads (maxEE will tolerate up to 6 errors). Instead they rely
## on mergePairs() to drop read pairs if they have >1 mismatch in their
## overlapping 3' regions (which should be much longer than the 12 nt
## minOverlap!). For the 1 tolerated mismatch, let mergePairs() determine which
## read is correct based on quality and abundance.
##    -- Note that 6 errors in a 325 nt merged read for nifH would be 1.85%
##       errors.  These errors would seem ulikely in the overlapping region
##       (>150nt in the few data sets I've worked with) and also unlikely in the
##       non-overlapping, more 5' regions because they are higher quality.
##       Probably 1.85% is pessimistic -- and that is for one read pair, i.e.
##       before DADA2 "denoises" over many read pairs when inferring the ASVs.


## Parameters that impact DADA2 error models:  Only R1 reads that appear NifH-like
## are used to create DADA2 error models. These reads are identified before
## the main DADA2 stage runs, using several scripts that invoke HMMER and filter
## the results. Filtering includes accepting only ORFs/reads with alignments
## longer than the min length specified here, and similar for bit score. (The
## default values are shown here, so the lines can be commented out.)
#NifH_minLen,  33
#NifH_minBits,150


## You can specify primers that will be trimmed by cutadapt. Specify both from
## 5' to 3'.  The default primers assumed by runCutadapt.sh are shown (and will
## be used if you omit the next two lines).
forward,TGYGAYCCNAARGCNGA
reverse,ADNGCCATCATYTCNCC

## These are for filterAndTrim().  Here you can decide between two strategies
## for trimming the reads.
##
##  (A) You can rely only on truncQ so that each forward and each reverse read
##      will be trimmed at the first base to to drop below the specified Phred
##      score.
##  (B) You can set fixed trim lengths for the forward and reverse reads
##      (truncLen.fwd and truncLen.rev) based on inspection of average quality
##      of the reads in e.g. FastQC reports.
##
## "B" is more commonly used, indeed DADA2's default filterAndTrim() by default
## uses a very low score for truncQ (Phred = 2).  Perhaps there are cases where
## "A" is useful for aggressively quality filtering reads, but at the cost of
## throwing out much of the data and making diversity analysis impossible.  In
## practice I have found that option "A" with a high truncQ throws out much data
## and yields fewer ASVs, while option "A" with a lower truncQ (e.g. 9)
## encourages reads to overlap which leads to DADA2 finding many of the same
## ASVs it would if you just used approach "B."
truncQ,16
##truncLen.fwd,XXX  <-- replace based on your R1 qualities
##truncLen.rev,YYY  <-- replace based on your R2 qualities
maxEE.fwd,2
maxEE.rev,4
minLen,20

## Two other options:
## -- You can also use just the R1 reads for your ASVs if the R2's are very low quality.
##useOnlyR1Reads,TRUE
## -- You can tell DADA2 to generate ASVs that do not merge the left and right
##    sequences. (The "sequences" are actually left and right ASVs, not reads.)
##    Instead you will get ASVS the left and right parts separated by 10 N's.
##justConcatenate,TRUE


## These are for mergePairs(). minOverlap 12 is the default. maxMismatch default
## is 0 but I prefer 1 as described above.
minOverlap,12
maxMismatch,1
